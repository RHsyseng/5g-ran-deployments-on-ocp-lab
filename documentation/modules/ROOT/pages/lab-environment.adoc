= Lab Environment
include::_attributes.adoc[]
:profile: 5g-ran-lab

This section describes how to deploy your own lab environment.

CAUTION: If you are a Red Hatter, you can order a lab environment on the https://demo.redhat.com[Red Hat Demo Platform]. You just need to order the lab named `5G RAN Deployment on OpenShift`.

[#lab-requirements]
== Lab Requirements

RHEL 8.X box with access to the Internet. This lab relies on KVM, so you need to have the proper virtualization packages already installed. It is highly recommended to use a bare-metal host. Our lab environment has the following specs:

* 64 cores in total, with or without hyperthreading enabled.
* 200GiB Memory.
* 1 TiB storage.

IMPORTANT: These instructions have been tested in a RHEL 8.7, we cannot guarantee that other operating systems (even RHEL-based) will work. We won't be providing support out of RHEL 8.

These are the steps to install the required packages on a RHEL 8.7 server:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm
usermod -aG qemu,libvirt $(id -un)
newgrp libvirt
systemctl enable --now libvirtd
-----

[#lab-deployment]
== Lab Deployment

IMPORTANT: All the steps in the below sections must be run as `root` user on the hypervisor host.

[#install-kcli]
=== Install kcli

We use https://github.com/karmab/kcli[kcli] to do several things, like managing VMs, deploying the first OCP cluster, etc. Additional kcli documentation can be found at `https://kcli.readthedocs.io`

IMPORTANT: Below commands must be executed from the hypervisor host as root if not specified otherwise.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
dnf -y copr enable karmab/kcli
dnf -y install kcli bash-completion vim jq tar git ipcalc python3-pyOpenSSL
-----

[#install-oc-kubectl]
=== Install oc/kubectl CLIs

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
kcli download oc -P version=stable -P tag='4.13'
kcli download kubectl -P version=stable -P tag='4.13'
mv kubectl oc /usr/bin/
-----

[#configure-disconnected-network]
=== Configure Disconnected Network

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
kcli create network -c 192.168.125.0/24 --nodhcp --domain 5g-deployment.lab 5gdeploymentlab
-----

[#configure-local-dns-dhcp-server]
=== Configure Local DNS/DHCP Server

NOTE: If you're using MacOS and you're getting errors while running `sed -i` commands, make sure you are using `gnu-sed`: `brew install gnu-sed`.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
dnf -y install dnsmasq policycoreutils-python-utils
mkdir -p /opt/dnsmasq/include.d/
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/dnsmasq.conf -o /opt/dnsmasq/dnsmasq.conf
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/upstream-resolv.conf -o /opt/dnsmasq/upstream-resolv.conf
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/hub.ipv4 -o /opt/dnsmasq/include.d/hub.ipv4
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/sno1.ipv4 -o /opt/dnsmasq/include.d/sno1.ipv4
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/sno2.ipv4 -o /opt/dnsmasq/include.d/sno2.ipv4
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/infrastructure-host.ipv4 -o /opt/dnsmasq/include.d/infrastructure-host.ipv4
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/dnsmasq/dnsmasq-virt.service -o /etc/systemd/system/dnsmasq-virt.service
touch /opt/dnsmasq/hosts.leases
semanage fcontext -a -t dnsmasq_lease_t /opt/dnsmasq/hosts.leases
restorecon /opt/dnsmasq/hosts.leases
sed -i "s/UPSTREAM_DNS/1.1.1.1/" /opt/dnsmasq/upstream-resolv.conf
systemctl daemon-reload
systemctl enable --now dnsmasq-virt
systemctl mask dnsmasq
-----

[#configure-local-dns-as-primary-server]
=== Configure Local DNS as Primary Server

The default upstream DNS is set to 1.1.1.1 in `/opt/dnsmasq/upstream-resolv.conf`. There might be cases in your local environment where the hypervisor may not reach it. So, notice that you must change it to a different DNS that allows you to resolve public hostnames. Once changed, remember to restart the dnsmasq-virt service.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
curl -L https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/hypervisor/forcedns -o /etc/NetworkManager/dispatcher.d/forcedns
chmod +x /etc/NetworkManager/dispatcher.d/forcedns
systemctl restart NetworkManager
/etc/NetworkManager/dispatcher.d/forcedns
-----

[#disable-firewall]
=== Disable Firewall

You can also create the required rules in the firewall if you want, but for the sake of simplicity we are disabling the firewall.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
systemctl disable firewalld iptables
systemctl stop firewalld iptables
iptables -F
-----

[#configure-webcache]
=== Configure Webcache

The webcache is basically an Apache httpd container serving the RHCOS live ISO and rootfs images locally. They are required for provisioning OCP clusters.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
curl -L https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/webcache/podman-webcache.service -o /etc/systemd/system/podman-webcache.service
mkdir -p /opt/webcache/data
curl -L https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.13/4.13.0/rhcos-4.13.0-x86_64-live-rootfs.x86_64.img -o /opt/webcache/data/rhcos-4.13.0-x86_64-live-rootfs.x86_64.img
curl -L https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.13/4.13.0/rhcos-4.13.0-x86_64-live.x86_64.iso -o /opt/webcache/data/rhcos-4.13.0-x86_64-live.x86_64.iso
systemctl daemon-reload
systemctl enable podman-webcache --now
-----

[#install-ksushytool]
=== Install Ksushy Tool

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
pip3 install cherrypy
kcli create sushy-service --ssl --port 9000
systemctl enable ksushy --now
-----

[#configure-disconnected-registry]
=== Configure Disconnected Registry

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
dnf -y install podman httpd-tools
REGISTRY_NAME=infra.5g-deployment.lab
mkdir -p /opt/registry/{auth,certs,data,conf}
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/registry/registry-key.pem -o /opt/registry/certs/registry-key.pem
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/registry/registry-cert.pem -o /opt/registry/certs/registry-cert.pem
htpasswd -bBc /opt/registry/auth/htpasswd admin r3dh4t1! 
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/registry/config.yml -o /opt/registry/conf/config.yml
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/registry/podman-registry.service -o /etc/systemd/system/podman-registry.service
systemctl daemon-reload
systemctl enable podman-registry --now
cp /opt/registry/certs/registry-cert.pem /etc/pki/ca-trust/source/anchors/
update-ca-trust
podman login --authfile auth.json -u admin  infra.5g-deployment.lab:8443 -p r3dh4t1!
-----

NOTE: Link to additional configuration documentation of a disconnected registry: https://docs.openshift.com/container-platform/4.13/installing/disconnected_install/installing-mirroring-disconnected.html

[#configure-git-server]
=== Configure Git Server

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
mkdir -p /opt/gitea/
chown -R 1000:1000 /opt/gitea/
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/gitea/podman-gitea.service -o /etc/systemd/system/podman-gitea.service
systemctl daemon-reload
systemctl enable podman-gitea --now
podman exec --user 1000 gitea /bin/sh -c 'gitea admin user create --username student --password student --email student@5g-deployment.lab --must-change-password=false --admin'
curl -u 'student:student' -H 'Content-Type: application/json' -X POST --data '{"service":"2","clone_addr":"https://github.com/RHsyseng/5g-ran-deployments-on-ocp-lab.git","uid":1,"repo_name":"5g-ran-deployments-on-ocp-lab"}' http://infra.5g-deployment.lab:3000/api/v1/repos/migrate
curl -u 'student:student' -H 'Content-Type: application/json' -X POST --data '{"service":"2","clone_addr":"https://github.com/RHsyseng/5g-ran-lab-aap-integration-tools.git","uid":1,"repo_name":"aap-integration-tools"}' http://infra.5g-deployment.lab:3000/api/v1/repos/migrate
-----

WARNING: It could be possible that the last few commands do not work at the first time since the registry container image is being pulled and then started. If that's the case, try them once again after a couple of seconds.

[#configure-ntp-server]
=== Configure NTP Server

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
dnf install chrony -y
cat <<EOF > /etc/chrony.conf
server time.cloudflare.com iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
keyfile /etc/chrony.keys
leapsectz right/UTC
logdir /var/log/chrony
bindcmdaddress ::
allow 192.168.125.0/24
EOF
systemctl enable chronyd --now
-----

[#configure-access-to-cluster-apps]
=== Configure Access to Cluster Apps

In order to access the hub cluster we will deploy an HAProxy that will be listening on the public interface of the Hypervisor host.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
dnf install haproxy -y
semanage port -a -t http_port_t -p tcp 6443
curl -L https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/haproxy/haproxy.cfg -o /etc/haproxy/haproxy.cfg
systemctl enable haproxy --now
-----

After that you need to add the following entries to your /etc/hosts file:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
<HYPERVISOR_REACHABLE_IP> infra.5g-deployment.lab api.hub.5g-deployment.lab multicloud-console.apps.hub.5g-deployment.lab console-openshift-console.apps.hub.5g-deployment.lab oauth-openshift.apps.hub.5g-deployment.lab openshift-gitops-server-openshift-gitops.apps.hub.5g-deployment.lab api.sno1.5g-deployment.lab api.sno2.5g-deployment.lab
-----

[#create-openshift-nodes-vms]
=== Create SNO Nodes VMs

Before running the following commands, make sure you have generated a SSH key pair in your default location `~/.ssh/`. That SSH key will allow you to connect to the VMs you are about to create:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
kcli create pool -p /var/lib/libvirt/images default
kcli create vm -P start=False -P uefi_legacy=true -P plan=hub -P memory=24000 -P numcpus=12 -P disks=[200,200] -P nets=['{"name": "5gdeploymentlab", "mac": "aa:aa:aa:aa:02:01"}'] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201 -P name=sno1
kcli create vm -P start=False -P uefi_legacy=true -P plan=hub -P memory=24000 -P numcpus=12 -P disks=[200,200] -P nets=['{"name": "5gdeploymentlab", "mac": "aa:aa:aa:aa:03:01"}'] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0301 -P name=sno2
-----

If you need or want to connect to any of the VMs you can do so by just executing:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
kcli ssh <VM_name>
-----


[#deploy-openshift-hub-cluster]
=== Deploy OpenShift Hub Cluster

IMPORTANT: This step requires a valid OpenShift Pull Secret placed in /root/openshift_pull.json. Notice that you can replace the admin or developer's password shown below for any other.

NOTE: If you're using MacOS and you're getting errors while running `sed -i` commands, make sure you are using `gnu-sed`: `brew install gnu-sed`.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
curl -sL https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/hub-cluster/hub.yml -o /root/hub.yml
sed -i "s/CHANGE_ADMIN_PWD/admin/" hub.yml
sed -i "s/CHANGE_DEV_PWD/developer/" hub.yml
cd /root/
kcli create cluster openshift --pf hub.yml
-----

This will take around 40 minutes to complete.

If the installation fails for whatever reason, you will need to delete all the VMs that were created and execute the same procedure again. So, first remove the plans, which actually will remove all VMs:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----

# kcli list plan
+------+--------------------------------------------------------+
| Plan |                          Vms                           |
+------+--------------------------------------------------------+
| hub  | hub-ctlplane-0,hub-ctlplane-1,hub-ctlplane-2,sno1,sno2 |
+------+--------------------------------------------------------+

# kcli delete plan hub -y
hub-ctlplane-0 deleted on local!
hub-ctlplane-1 deleted on local!
hub-ctlplane-2 deleted on local!
sno1 deleted on local!
sno2 deleted on local!
Plan hub deleted!
-----

And then create the VMs again as explained in the previous section link:lab-environment.html#create-openshift-nodes-vms[Deploy OpenShift Hub Cluster].


[#configure-openshift-hub-cluster]
=== Configure OpenShift Hub Cluster

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
export KUBECONFIG=~/.kcli/clusters/hub/auth/kubeconfig
oc -n openshift-storage wait lvmcluster lvmcluster --for=jsonpath='{.status.state}'=Ready --timeout=900s
oc patch storageclass lvms-vg1 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
curl https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/hub-cluster/argocd-patch.json -o /tmp/argopatch.json
oc patch argocd openshift-gitops -n openshift-gitops --type=merge --patch-file /tmp/argopatch.json
oc wait --for=condition=Ready pod -lapp.kubernetes.io/name=openshift-gitops-repo-server -n openshift-gitops
oc -n openshift-gitops adm policy add-cluster-role-to-user cluster-admin -z openshift-gitops-argocd-application-controller
-----

[#deploy-openshift-hub-cluster-operators]
=== Deploy OpenShift HUB Cluster Operators

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc apply -f https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/hub-cluster/hub-operators-argoapps.yaml
-----

[#deploy-sno1-cluster-without-ztp]
=== Deploy SNO1 Cluster (without ZTP)

A SNO is deployed outside the ZTP workflow so students can import it and see how that workflow works.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
curl -L http://infra.5g-deployment.lab:3000/student/5g-ran-deployments-on-ocp-lab/raw/branch/lab-4.13/lab-materials/lab-env-data/hypervisor/ssh-key -o /root/.ssh/snokey
chmod 400 /root/.ssh/snokey
oc apply -f https://raw.githubusercontent.com/RHsyseng/5g-ran-deployments-on-ocp-lab/lab-4.13/lab-materials/lab-env-data/hub-cluster/sno1-argoapp.yaml
-----

Once the cluster is deployed, the kubeconfig can be gathered as follows:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc -n sno1 get agentclusterinstall,agent
NAME                                                    CLUSTER   STATE
agentclusterinstall.extensions.hive.openshift.io/sno1   sno1      adding-hosts

NAME                                                                    CLUSTER   APPROVED   ROLE     STAGE
agent.agent-install.openshift.io/aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201   sno1      true       master   Done

oc extract secret/sno1-admin-kubeconfig --to=- -n sno1 > /root/sno1kubeconfig
oc --kubeconfig /root/sno1kubeconfig get nodes,clusterversion

NAME                      STATUS   ROLES                         AGE     VERSION
node/openshift-master-0   Ready    control-plane,master,worker   94m   v1.26.5+0001a21

NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   4.13.2    True        False         75m     Cluster version is 4.13.2
-----


// [#configure-aap]
// === Configure AAP
// 
// NOTE: These steps require `awx` CLI
// 
// [.console-input]
// [source,bash]
// -----
// pip3 install awxkit
// awx --help
// -----
// 
// First wait for AutomationHub and AutomationController to finish deployment.
// 
// Next, retrieve Automation Hub and Automation Controller credentials, urls, and tokens. In order to do this, we will need to add the following entry to `/etc/hosts` then run the commands.
// 
// [.console-input]
// [source,bash]
// -----
// <HYPERVISOR_REACHABLE_IP> automation-hub-aap.apps.hub.5g-deployment.lab automation-aap.apps.hub.5g-deployment.lab
// -----
// 
// [.console-input]
// [source,bash]
// -----
// export GALAXY_NG_USER="admin"
// export GALAXY_NG_PASSWORD=$(oc -n aap get secret automation-hub-admin-password -o jsonpath='{.data.password}' | base64 -d)
// export ANSIBLE_CTRL_USER="admin"
// export ANSIBLE_CTRL_PASSWORD=$(oc -n aap get secret automation-admin-password -o jsonpath='{.data.password}' | base64 -d)
// export GALAXY_NG_URL="https://$(oc -n aap get route automation-hub -o jsonpath='{.spec.host}')"
// export ANSIBLE_CTRL_URL="https://$(oc -n aap get route automation -o jsonpath='{.spec.host}')"
// curl -sku ${GALAXY_NG_USER}:"${GALAXY_NG_PASSWORD}" -X POST ${GALAXY_NG_URL}/api/galaxy/v3/auth/token/ | jq -r .token > /opt/galaxy_token
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure login | jq -r .token > /opt/tower_token
// -----
// 
// Now, we upload collections to our private Automation Hub.
// 
// We will create a requirements.yml file that holds the names and versions for each Ansible collection needed for this lab.
// 
// [.console-input]
// [source,bash]
// -----
// cat << EOF > ~/requirements.yml -
// collections:
// - name: kubernetes.core
//   version: 2.4.0
// - name: awx.awx
//   version: 22.5.0
// - name: stolostron.core
//   version: 0.0.2
// EOF
// -----
// 
// For each collection specified in requirements.yml, we need to download the collection and its dependencies as a tarball from link:https://galaxy.ansible.com[Ansible Galaxy], create a namespace on our private Automation Hub to host the collection, upload the collection to the hub, then approve the collection for use by external components. We can accomplish this via the ansible-galaxy command (part of Ansible's set of CLI tools) and the API exposed by the Automation Hub.
// 
// First, let's download all required collection tarballs to a directory called ansible-collections.
// 
// NOTE: If the `ansible-galaxy` can not be found, run `python3 -m pip install --user ansible` and add `~/.local/bin` to your PATH.
// 
// [.console-input]
// [source,bash]
// -----
// ansible-galaxy collection download -r ~/requirements.yml -p ~/ansible-collections
// -----
// 
// Now for each downloaded collection, we create a namespace for the collection, publish the collection to the Automation Hub, then approve the collection.
// 
// Given a downloaded ansible collection in the ansible-collections directory with namespace NAMESPACE, collection name COLLECTION, and version VERSION, the process is as follows:
// 
// //[.console-input]
// [source,bash]
// -----
// # create namespace and get API response
// curl -sku ${GALAXY_NG_USER}:"${GALAXY_NG_PASSWORD}" -X POST -H 'Content-Type: application/json' -d '{"name":"'$NAMESPACE'","groups":[]}' ${GALAXY_NG_URL}/api/galaxy/_ui/v1/namespaces/ | jq
// 
// # publish collection using ansible-galaxy command
// # -s flag points ansible-galaxy to our private Automation Hub, --token flag specifies our Automation Hub token
// ansible-galaxy collection publish "~/ansible-collections/${NAMESPACE}-${COLLECTION}-${VERSION}.tar.gz" -s ${GALAXY_NG_URL}/api/galaxy/ --token $(cat /opt/galaxy_token) --ignore-certs
// 
// # approve collection and get API response
// curl -sku ${GALAXY_NG_USER}:"${GALAXY_NG_PASSWORD}" -X POST -H 'Content-Type: application/json' "${GALAXY_NG_URL}"/api/galaxy/v3/collections/"${NAMESPACE}"/"${COLLECTION}"/versions/"${VERSION}"/move/staging/published/ | jq
// -----
// 
// This process is easily automatable with a simple bash script:
// 
// [.console-input]
// [source,bash,subs="attributes+,+macros"]
// -----
// curl https://raw.githubusercontent.com/RHsyseng/5g-ran-lab-aap-integration-tools/main/collections/collections_to_hub.bash -o ~/collections_to_hub.bash
// -----
// 
// [source,bash]
// -----
// > cat ~/collections_to_hub.bash
// #!/bin/bash
// 
// [ -z "$GALAXY_NG_USER" ] && echo "GALAXY_NG_USER is not set"
// [ -z "$GALAXY_NG_PASSWORD" ] && echo "GALAXY_NG_PASSWORD is not set"
// [ -z "$GALAXY_NG_URL" ] && echo "GALAXY_NG_URL is not set"
// [ ! -f "/opt/galaxy_token" ] && echo "Expected galaxy token at /opt/galaxy_token"
// 
// DIR=~/ansible-collections
// 
// for path in ${DIR}/*.tar.gz; do
//     filename=$(basename ${path} .tar.gz)
//     readarray -d "-" -t filearray <<< "$filename"
//     NAMESPACE=${filearray[0]}
//     COLLECTION=${filearray[1]}
//     VERSION=$(echo "${filearray[2]}" | tr -d '\n')
//     echo "Uploading ${NAMESPACE}-${COLLECTION}-${VERSION}"
//     #create namespace
//     curl -sku ${GALAXY_NG_USER}:"${GALAXY_NG_PASSWORD}" -X POST -H 'Content-Type: application/json' -d '{"name":"'${NAMESPACE}'","groups":[]}' ${GALAXY_NG_URL}/api/galaxy/_ui/v1/namespaces/ | jq
//     #publish collection
//     ansible-galaxy collection publish "${DIR}/${NAMESPACE}-${COLLECTION}-${VERSION}.tar.gz" -s ${GALAXY_NG_URL}/api/galaxy/ --token $(cat /opt/galaxy_token) --ignore-certs
//     #approve collection
//     curl -sku ${GALAXY_NG_USER}:"${GALAXY_NG_PASSWORD}" -X POST -H 'Content-Type: application/json' "${GALAXY_NG_URL}"/api/galaxy/v3/collections/"${NAMESPACE}"/"${COLLECTION}"/versions/"${VERSION}"/move/staging/published/ | jq
// done
// -----
// 
// [.console-input]
// [source,bash]
// -----
// chmod u+x ~/collections_to_hub.bash
// ~/collections_to_hub.bash
// -----
// 
// After running this script, if we head to automation-hub-aap.apps.hub.5g-deployment.lab and click on the "Collections" tab, we will see the collections we have just uploaded. Note that the `amazon.aws` collection has been included, which is a listed dependency of `stolostron.core` that `ansible-galaxy` has taken care of.
// 
// image::aap-hub-collections.png[AAP Hub Collections]
// 
// Now that we have configured Automation Hub, we will configure Automation Controller next.
// 
// First, we need to attach Automation Controller to a subscription. Either log into Automation Controller at https://automation-aap.apps.hub.5g-deployment.lab and select a subscription via web GUI, or if you have an existing manifest zip file, you can run:
// 
// [.console-input]
// [source,bash]
// -----
// echo "{\"manifest\": \"$(base64 -w 0 </path/to/manifest.zip>)\"}" > ~/manifest_payload.json
// curl -vk -u $ANSIBLE_CTRL_USER:$ANSIBLE_CTRL_PASSWORD -H 'Content-Type:application/json' -X POST -d @~/manifest_payload.json "${ANSIBLE_CTRL_URL}/api/v2/config/"
// -----
// 
// Our Automation Controller should now be configured with a subscription.
// 
// Next, lets create authorization credentials for accessing our OpenShift Hub. We will start off by creating a credential type with custom inputs and injectors, then create a credential of that credential type.
// 
// [.console-input]
// [source,bash,subs="+macros,attributes+"]
// -----
// mkdir ~/5g-deployment-lab
// curl https://raw.githubusercontent.com/RHsyseng/5g-ran-lab-aap-integration-tools/main/tower-config/credential_inputs_kubeconfig.json -o ~/5g-deployment-lab/credential_inputs_kubeconfig.json
// curl https://raw.githubusercontent.com/RHsyseng/5g-ran-lab-aap-integration-tools/main/tower-config/credential_injector_kubeconfig.json -o ~/5g-deployment-lab/credential_injector_kubeconfig.json
// awx --conf.host pass:[${ANSIBLE_CTRL_URL}] --conf.username pass:[${ANSIBLE_CTRL_USER}] --conf.password pass:["${ANSIBLE_CTRL_PASSWORD}"] --conf.insecure credential_types create --name 'kubeconfig' --kind cloud --inputs '@~/5g-deployment-lab/credential_inputs_kubeconfig.json' --injectors '@~/5g-deployment-lab/credential_injector_kubeconfig.json'
// awx --conf.host pass:[${ANSIBLE_CTRL_URL}] --conf.username pass:[${ANSIBLE_CTRL_USER}] --conf.password pass:["${ANSIBLE_CTRL_PASSWORD}"] --conf.insecure credentials create --name 'hub-acm-kubeconfig' --organization 'Default' --credential_type 'kubeconfig' --inputs "{'kube_config':'@~/.kcli/clusters/hub/auth/kubeconfig'}"
// -----
// 
// Next, we'll create authorization credentials to give our AAP Controller access to the AAP Hub. Since we are using self-signed certificates for the AAP Hub, we will also need to disable SSL certificate verifications.
// 
// [.console-input]
// [source,bash]
// -----
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure credentials create --name 'hub-galaxy-token' --organization 'Default' --credential_type 'Ansible Galaxy/Automation Hub API Token' --inputs "{'url':'$GALAXY_NG_URL/api/galaxy/content/published/','token':'$(cat /opt/galaxy_token)'}"
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure settings modify GALAXY_IGNORE_CERTS true
// -----
// 
// Since we wish to pull collections from our private Automation Hub, we will configure the Automation Controller default organization to use our custom Automation Hub credentials created above instead of `galaxy.ansible.com` credentials.
// 
// [.console-input]
// [source,bash]
// -----
// awx --conf.host "${ANSIBLE_CTRL_URL}" --conf.username "${ANSIBLE_CTRL_USER}" --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure organizations disassociate --organization 'Default' --galaxy_credential 'Ansible Galaxy'
// awx --conf.host "${ANSIBLE_CTRL_URL}" --conf.username "${ANSIBLE_CTRL_USER}" --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure organizations associate --organization 'Default' --galaxy_credential 'hub-galaxy-token'
// -----
// 
// There is a hypervisor hosted git repository at https://infra.5g-deployment.lab:3000/student/aap-integration-tools/. This repository contains relevant configuration files that we will be using for AAP automation. We will be using the Ansible Controller's source control management feature to manage our automation file and automatically update our automation project when the source is updated.
// 
// We will create an Ansible project using this git repository as our target.
// 
// [.console-input]
// [source,bash,subs="+macros,attributes+"]
// -----
// awx --conf.host pass:[${ANSIBLE_CTRL_URL}] --conf.username pass:[${ANSIBLE_CTRL_USER}] --conf.password pass:["${ANSIBLE_CTRL_PASSWORD}"] --conf.insecure projects create --name 'aap-for-ran-ztp-project' --organization 'Default' --wait --scm_type git --scm_url http://infra.5g-deployment.lab:3000/student/aap-integration-tools.git --scm_branch main
// -----
// 
// Since each Ansible deployment needs a list of hosts it can run on, we will create an inventory along with a dynamic inventory source.
// 
// [.console-input]
// [source,bash]
// -----
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure inventory create --name 'hub-acm-inventory' --organization 'Default'
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure inventory_sources create --name 'acm-dynamic-inventory' --organization 'Default' --inventory 'hub-acm-inventory' --update_on_launch true --credential 3 --source scm --source_project 'aap-for-ran-ztp-project' --source_path 'inventories/cluster-inventory-example.yml'
// -----
// 
// NOTE: The `--credential 3` flag when creating our inventory source refers to the `hub-acm-kubeconfig` credential that we created earler, where 3 is the id of the credential. We can see the credentials we've created in json format along with information like the credential id by running:
// 
// [.console-input]
// [source,bash]
// -----
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure credentials list
// -----
// 
// Now, we create our job template and associate the template with our ACM Hub credentials. This is the job template for the Ansible job that gets invoked by ACM. The jobe template we create is associated with our created project and inventory, a playbook to run in our git repository, and any extra variables we wish to add as input.
// 
// [.console-input]
// [source,bash]
// -----
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure job_templates create --name 'ztp-day2-automation-template' --project 'aap-for-ran-ztp-project' --playbook 'playbooks/cluster_mgmt_example_playbook.yml' --inventory 'hub-acm-inventory' --ask_variables_on_launch true --allow_simultaneous true --extra_vars '{"state": "present", "namespace_to_add": "ztp-day2-automation"}'
// awx --conf.host ${ANSIBLE_CTRL_URL} --conf.username ${ANSIBLE_CTRL_USER} --conf.password "${ANSIBLE_CTRL_PASSWORD}" --conf.insecure job_templates associate 'ztp-day2-automation-template' --credential 'hub-acm-kubeconfig'
// -----
// 
// Let's patch MultiClusterHub with the necessary add-ons to enable our OpenShift Hub to access and configure spoke clusters.
// 
// [.console-input]
// [source,bash]
// -----
// oc patch mch multiclusterhub -n open-cluster-management --type=json -p='[{"op": "add", "path": "/spec/overrides/components/-","value":{"name":"cluster-proxy-addon","enabled":true}}]'
// oc patch mch multiclusterhub -n open-cluster-management --type=json -p='[{"op": "add", "path": "/spec/overrides/components/-","value":{"name":"managedserviceaccount-preview","enabled":true}}]'
// ----- 
// 