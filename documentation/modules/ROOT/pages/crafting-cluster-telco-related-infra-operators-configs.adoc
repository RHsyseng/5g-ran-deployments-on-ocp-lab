= Crafting the Cluster Telco Related Infrastructure Operators Configs
include::_attributes.adoc[]
:profile: 5g-ran-lab

In the previous section, we created the infrastructure as code (IaC) for our `SNO2` cluster using the `SiteConfig` object. In this section, we are going to create the configuration as code (CaC) for our clusters by using multiple `PolicyGenTemplate` objects.

We described how `PolicyGenTemplate` works in detail xref:policygen-deepdive.adoc#policies-templating[here], so let's jump directly to the creation of the different templates.

NOTE: The configuration defined through these PolicyGenTemplate CRs is only a subset of what was described in xref:telco-related-infra-operators-intro.adoc[Introduction to Telco Related Infrastructure Operators]. This is for clarity and tailored to the lab environment. A full production environment for supporting telco 5G vRAN workloads would have additional configuration not included here, but described in detail as the {rds-link}[{openshift-release} Telco RAN Reference Design Specification] slide deck.

IMPORTANT: Below commands must be executed from the workstation host if not specified otherwise.

[#crafting-common-policies]
== Crafting Common Policies

The common policies apply to every cluster in our infrastructure that matches our binding rule. These policies are often used to configure things like CatalogSources, operator deployments, etc. that are common to all our clusters.

These configs may vary from release to release, that's why we create a `{policygen-common-file}` file. We will likely have a common configuration profile for each release we deploy.

IMPORTANT: If you check the binding rules, you see that we are targeting clusters labeled with `common: "{policygen-common-label}"` and `logicalGroup: "active"`. These labels were set in the `SiteConfig` definition in the link:crafting-deployments-iaac.html#siteconfig[previous section].

1. Create the `common` PolicyGenTemplate for {openshift-release} SNOs:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/{policygen-common-file}
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "common"
  namespace: "ztp-policies"
spec:
  bindingRules:
    common: "{policygen-common-label}"
    logicalGroup: "active"
  mcp: master
  remediationAction: inform
  sourceFiles:
    - fileName: DefaultCatsrc.yaml
      metadata:
        name: redhat-operator-index
      spec:
        image: infra.5g-deployment.lab:8443/redhat/redhat-operator-index:{catalogsource-index-image-tag}
      policyName: config-policy
    - fileName: ReduceMonitoringFootprint.yaml
      policyName: config-policy
    - fileName: StorageLVMSubscriptionNS.yaml
      metadata:
        annotations:
          workload.openshift.io/allowed: management
      policyName: subscriptions-policy
    - fileName: StorageLVMSubscriptionOperGroup.yaml
      policyName: subscriptions-policy
    - fileName: StorageLVMSubscription.yaml
      spec:
        channel: {lvms-channel}
        source: redhat-operator-index
      policyName: subscriptions-policy
    - fileName: LVMOperatorStatus.yaml
      policyName: subscriptions-policy
EOF
-----

[#crafting-group-policies]
== Crafting Group Policies

The group policies apply to a group of clusters that typically have something in common, for example they are SNOs, or they have similar hardware: SR-IOV cards, number of CPUs, etc.

The CNF team has prepared some common tuning configurations that should be applied on every SNO DU deployed with similar hardware. In this section, we will be crafting these configurations.

IMPORTANT: If you check the binding rules you can see that we are targeting clusters labeled with `group-du-sno: ""` and `logicalGroup: "active"`. These labels were set in the `SiteConfig` definition in the link:crafting-deployments-iaac.html#siteconfig[previous section].

1. Create the `group` PolicyGenTemplate for SNOs:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/group-du-sno.yaml
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-sno"
  namespace: "ztp-policies"
spec:
  bindingRules:
    group-du-sno: ""
    logicalGroup: "active"
    hardware-type: "hw-type-platform-1"
  mcp: master
  remediationAction: inform
  sourceFiles:
    - fileName: DisableSnoNetworkDiag.yaml
      policyName: "group-policy"
    - fileName: DisableOLMPprof.yaml # wave 10
      policyName: "group-policy"
    - fileName: PerformanceProfile.yaml
      policyName: "group-policy"
      metadata:
        annotations:
          kubeletconfig.experimental: |
            {"topologyManagerScope": "pod",
             "systemReserved": {"memory": "3Gi"}
            }
      spec:
        cpu:
          isolated: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-isolated" (index .ManagedClusterLabels "hardware-type")) hub}}'
          reserved: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-reserved" (index .ManagedClusterLabels "hardware-type")) hub}}'
        hugepages:
          defaultHugepagesSize: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-default" (index .ManagedClusterLabels "hardware-type"))| hub}}'
          pages:
          - count: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-count" (index .ManagedClusterLabels "hardware-type")) | toInt hub}}'
            size: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-size" (index .ManagedClusterLabels "hardware-type")) hub}}'
        numa:
          topologyPolicy: single-numa-node
        realTimeKernel:
          enabled: false
        globallyDisableIrqLoadBalancing: false
        # WorkloadHints defines the set of upper level flags for different type of workloads.
        # The configuration below is set for a low latency, performance mode.
        workloadHints:
          realTime: true
          highPowerConsumption: false
          perPodPowerManagement: false
    - fileName: TunedPerformancePatch.yaml
      policyName: "group-policy"
      spec:
        profile:
          - name: performance-patch
            data: |
              [main]
              summary=Configuration changes profile inherited from performance created tuned
              include=openshift-node-performance-openshift-node-performance-profile
              [sysctl]
              # When using the standard (non-realtime) kernel, remove the kernel.timer_migration override from the [sysctl] section
              # kernel.timer_migration=0
              [scheduler]
              group.ice-ptp=0:f:10:*:ice-ptp.*
              group.ice-gnss=0:f:10:*:ice-gnss.*
              [service]
              service.stalld=start,enable
              service.chronyd=stop,disable
EOF
-----
IMPORTANT: By leveraging hub site templating we are reducing the number of policies on our hub cluster. This makes the clusters's deployment and maintenance process simpler. Notice that with large fleets of clusters, it can quickly become hard to maintain per-site configurations.

2. We're using policy templating, so we need to create a `ConfigMap` with the templating values to be used. Notice that the following resource contains values for multiple hardware specifications that we may have in our infrastructure. In the policy we have just applied we are obtaining the value for tuning our SNO by obtaining the value of the label `hardware-type` that each cluster was assigned to. This label was set in the `SiteConfig` definition in the link:crafting-deployments-iaac.html#siteconfig[previous section]. More information about Template processing can be found in {rhacm-template-processing}[Red Hat Advanced Cluster Management documentation].
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/group-hardware-types-configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: group-hardware-types-configmap
  namespace: ztp-policies
  annotations:
    argocd.argoproj.io/sync-options: Replace=true
data:  
  # PerformanceProfile.yaml
  hw-type-platform-1-cpu-isolated: "4-11"
  hw-type-platform-1-cpu-reserved: "0-3"
  hw-type-platform-1-hugepages-default: "1G"
  hw-type-platform-1-hugepages-count: "4"
  hw-type-platform-1-hugepages-size: "1G"
  hw-type-platform-2-cpu-isolated: "2-39,42-79"
  hw-type-platform-2-cpu-reserved: "0-1,40-41"
  hw-type-platform-2-hugepages-default: "256M"
  hw-type-platform-2-hugepages-count: "16"
  hw-type-platform-2-hugepages-size: "1G"  
EOF
-----
+
3. Since we are deploying DUs, we need to run the validator policies crafted by the CNF team as well:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/group-du-sno-validator.yaml
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-sno-validator"
  namespace: "ztp-policies"
spec:
  bindingRules:
    group-du-sno: ""
    logicalGroup: "active"
  bindingExcludedRules:
    ztp-done: ""
  mcp: "master"
  sourceFiles:
    - fileName: validatorCRs/informDuValidator.yaml
      remediationAction: inform
      policyName: "validation"
EOF
-----

[#crafting-site-policies]
== Crafting Site Policies

Site policies apply to a specific cluster/s in our site, they usually configure stuff that is very specific to a single cluster or to a small subset of clusters. For example, we could configure storage for a group of clusters if those share the same hardware, disks configurations, etc. otherwise we will need to have different bindings for different clusters.

We are going to create the storage configurations for our SNO2 cluster whose hardware type belongs to `hw-type-platform-1`. Notice that this policy leverages hub side templating. Therefore it is targeting all clusters with this specific hardware configuration, not only `SNO2`.

1. Create the `du-sno-sites` PolicyGenTemplate for the clusters that contain `hw-type-platform-1` hardware type in our `5glab` site. In our case the `SNO2` cluster:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/sites/hub-1/sites-specific.yaml
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-sno-sites"
  namespace: "ztp-policies"
spec:
  bindingRules:
    common: "{policygen-common-label}"
    logicalGroup: "active"
  mcp: master
  remediationAction: inform
  sourceFiles:
    - fileName: StorageLVMCluster.yaml
      spec:
        storage:
          deviceClasses:
            - name: vg1
              thinPoolConfig:
                name: thin-pool-1
                sizePercent: 90
                overprovisionRatio: 10
              deviceSelector:
                paths:
                - '{{hub fromConfigMap "" "site-data-configmap" (printf "%s-storage-path" .ManagedClusterName) hub}}'
      policyName: "sites-policy"
EOF
-----
IMPORTANT: By leveraging hub site templating we are reducing the number of policies on our hub cluster. This makes the clusters's deployment and maintenance process a lot less cumbersome. Notice that with large fleets of clusters, it can quickly become hard to maintain per-site configurations.

2. We're using policy templating, so we have to create a `ConfigMap` with the templating values to be used. Observe that the following manifest contains disk paths for each cluster deployed by our Hub-1. This storage information is required to configure the LVM operator and provide storage to our SNO clusters. In the previous policy, it is captured the device path used to present persistent storage to our applications by obtaining the name of the cluster managed by the hub. The information is not obtained from a label set in the `SiteConfig` definition, it is the `ManagedClusterName`, e.g., the name of the cluster used instead. 

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/sites/hub-1/site-data-hw-1-configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: site-data-configmap
  namespace: ztp-policies
  annotations:
    argocd.argoproj.io/sync-options: Replace=true
data:
  # StorageLVMCluster.yaml
  sno2-storage-path: "/dev/vdb"
  sno3-storage-path: "[\"/dev/disk/by-path/pci-0001:05:00.0-nvme-1\", \"pci-0000:18:00.0-scsi-0:2:0:0\"]"
EOF
-----

More information about Template processing can be found in {rhacm-template-processing}[Red Hat Advanced Cluster Management documentation].

[#crafting-testing-policies]
== Crafting testing Policies

Testing policies before applying them to our production clusters is a must, in order to do that we will create a set of testing policies (which usually will be very similar to the production ones) and these policies will target clusters labeled with `logicalGroup: "testing"`. We won't go over every file, if you check the files that will be created those are the same as in `active`, also known as `production`, but with different names and binding policies.

1. Create the `common` testing PolicyGenTemplate for {openshift-release} SNOs:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/testing/{policygen-common-file}
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "common-test"
  namespace: "ztp-policies"
spec:
  bindingRules:
    common: "{policygen-common-label}"
    logicalGroup: "testing"
  mcp: master
  remediationAction: inform
  sourceFiles:
    - fileName: DefaultCatsrc.yaml
      metadata:
        name: redhat-operator-index
      spec:
        image: infra.5g-deployment.lab:8443/redhat/redhat-operator-index:{catalogsource-index-image-tag}
      policyName: config-policy
    - fileName: ReduceMonitoringFootprint.yaml
      policyName: config-policy
    - fileName: StorageLVMSubscriptionNS.yaml
      metadata:
        annotations:
          workload.openshift.io/allowed: management
      policyName: subscriptions-policy
    - fileName: StorageLVMSubscriptionOperGroup.yaml
      policyName: subscriptions-policy
    - fileName: StorageLVMSubscription.yaml
      spec:
        channel: {lvms-channel}
        source: redhat-operator-index
      policyName: subscriptions-policy
    - fileName: LVMOperatorStatus.yaml
      policyName: subscriptions-policy
EOF
-----
+
2. Create the `group` testing PolicyGenTemplate for SNOs:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/testing/group-du-sno.yaml
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-sno-test"
  namespace: "ztp-policies"
spec:
  bindingRules:
    group-du-sno: ""
    logicalGroup: "testing"
    hardware-type: "hw-type-platform-1"
  mcp: master
  remediationAction: inform
  sourceFiles:
    - fileName: DisableSnoNetworkDiag.yaml
      policyName: "group-policy"
    - fileName: DisableOLMPprof.yaml # wave 10
      policyName: "group-policy"
    - fileName: PerformanceProfile.yaml
      policyName: "group-policy"
      metadata:
        annotations:
          kubeletconfig.experimental: |
            {"topologyManagerScope": "pod",
             "systemReserved": {"memory": "3Gi"}
            }
      spec:
        cpu:
          isolated: '{{hub fromConfigMap "" "group-hardware-types-configmap-test" (printf "%s-cpu-isolated" (index .ManagedClusterLabels "hardware-type")) hub}}'
          reserved: '{{hub fromConfigMap "" "group-hardware-types-configmap-test" (printf "%s-cpu-reserved" (index .ManagedClusterLabels "hardware-type")) hub}}'
        hugepages:
          defaultHugepagesSize: '{{hub fromConfigMap "" "group-hardware-types-configmap-test" (printf "%s-hugepages-default" (index .ManagedClusterLabels "hardware-type"))| hub}}'
          pages:
          - count: '{{hub fromConfigMap "" "group-hardware-types-configmap-test" (printf "%s-hugepages-count" (index .ManagedClusterLabels "hardware-type")) | toInt hub}}'
            size: '{{hub fromConfigMap "" "group-hardware-types-configmap-test" (printf "%s-hugepages-size" (index .ManagedClusterLabels "hardware-type")) hub}}'
        numa:
          topologyPolicy: single-numa-node
        realTimeKernel:
          enabled: false
        globallyDisableIrqLoadBalancing: false
        # WorkloadHints defines the set of upper level flags for different type of workloads.
        workloadHints:
          realTime: true
          highPowerConsumption: false
          perPodPowerManagement: false
    - fileName: TunedPerformancePatch.yaml
      policyName: "group-policy"
      spec:
        profile:
          - name: performance-patch
            data: |
              [main]
              summary=Configuration changes profile inherited from performance created tuned
              include=openshift-node-performance-openshift-node-performance-profile
              # When using the standard (non-realtime) kernel, remove the kernel.timer_migration override from the [sysctl] section
              [sysctl]
              kernel.timer_migration=0
              [scheduler]
              group.ice-ptp=0:f:10:*:ice-ptp.*
              group.ice-gnss=0:f:10:*:ice-gnss.*
              [service]
              service.stalld=start,enable
              service.chronyd=stop,disable

EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/testing/group-hardware-types-configmap-test.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: group-hardware-types-configmap-test
  namespace: ztp-policies
  annotations:
    argocd.argoproj.io/sync-options: Replace=true
data:  
  # PerformanceProfile.yaml
  hw-type-platform-1-cpu-isolated: "4-11"
  hw-type-platform-1-cpu-reserved: "0-3"
  hw-type-platform-1-hugepages-default: "1G"
  hw-type-platform-1-hugepages-count: "4"
  hw-type-platform-1-hugepages-size: "1G"
  hw-type-platform-2-cpu-isolated: "2-39,42-79"
  hw-type-platform-2-cpu-reserved: "0-1,40-41"
  hw-type-platform-2-hugepages-default: "256M"
  hw-type-platform-2-hugepages-count: "16"
  hw-type-platform-2-hugepages-size: "1G"  
EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/testing/group-du-sno-validator.yaml
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-sno-validator-test"
  namespace: "ztp-policies"
spec:
  bindingRules:
    group-du-sno: ""
    logicalGroup: "testing"
  bindingExcludedRules:
    ztp-done: ""
  mcp: "master"
  sourceFiles:
    - fileName: validatorCRs/informDuValidator.yaml
      remediationAction: inform
      policyName: "validation"
EOF
-----

At this point, policies are the same as in production (active). In the future, you prior want to apply these groups of testing policies for the clusters running in your test environment before promoting changes to production (active).

[#configure-kustomization-for-policies]
== Configure Kustomization for Policies

We need to create the required kustomization files as we did for SiteConfigs. In this case, policies also require a namespace where they will be created. Therefore, we will create the required namespace and the kustomization files.

1. Policies need to live in a namespace, let's add it to the repo:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/policies-namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: ztp-policies
  labels:
    name: ztp-policies
EOF
-----
+
2. Create the required Kustomization files
+
WARNING: You can see that we commented the `group-du-sno-validator.yaml` files in our Kustomization files. Since this is a lab environment and we don't have PTP/SR-IOV hardware, the validator policies won't be able to verify our SNOs as a well-configured DU. We kept the files here so you know how a real environment should be configured.
+

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - fleet/
  - sites/
  - policies-namespace.yaml
EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/sites/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
bases:
  - hub-1/
EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/sites/hub-1/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
generators:
  - sites-specific.yaml
resources:
  - site-data-hw-1-configmap.yaml
EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - active/
  - testing/
EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
generators:
  - {policygen-common-file}
  - group-du-sno.yaml
#  - group-du-sno-validator.yaml
resources:
  - group-hardware-types-configmap.yaml
EOF
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/testing/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
generators:
  - {policygen-common-file}
  - group-du-sno.yaml
#  - group-du-sno-validator.yaml
resources:
  - group-hardware-types-configmap-test.yaml
EOF
-----
+
3. At this point, we can push the changes to the repo and continue to the next section.
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cd ~/5g-deployment-lab/ztp-repository
git add --all
git commit -m 'Added policies information'
git push origin main
cd ~
-----
